# Inference API Configuration
INFERENCE_API_ENDPOINT=https://your-api-endpoint.com/deployment
INFERENCE_API_TOKEN=your-pre-generated-token-here
INFERENCE_MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

# Service Configuration
SERVICE_PORT=8002

# Model Settings
DEFAULT_TONE=conversational
DEFAULT_MAX_LENGTH=2000

# Generation Parameters
TEMPERATURE=0.7
MAX_TOKENS=4000
MAX_RETRIES=3
