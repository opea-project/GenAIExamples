# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

deploy:
  device: gaudi
  version: 1.2.0
  modelUseHostPath: /mnt/models
  HUGGINGFACEHUB_API_TOKEN: "" # mandatory
  node: [1, 2, 4, 8]
  namespace: ""

  services:
    backend:
      instance_num: [1, 2, 4, 8]
      cores_per_instance: ""
      memory_capacity: ""

    teirerank:
      enabled: True
      model_id: ""
      replicaCount: [1, 1, 1, 1]
      cards_per_instance: 1

    tei:
      model_id: ""
      replicaCount: [1, 2, 4, 8]
      cores_per_instance: ""
      memory_capacity: ""

    llm:
      engine: vllm  # or tgi
      model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
      replicaCount:
        with_teirerank: [7, 15, 31, 63]     # When teirerank.enabled is True
        without_teirerank: [8, 16, 32, 64]   # When teirerank.enabled is False
      # TGI specific settings
      max_batch_size: [1, 2, 4, 8]
      # VLLM specific settings
      max_num_seqs: [1, 2, 4, 8]
      # Common settings
      max_input_length: ""
      max_total_tokens: ""
      max_batch_total_tokens: ""
      max_batch_prefill_tokens: ""
      cards_per_instance: 1

    data-prep:
      replicaCount: [1, 1, 1, 1]
      cores_per_instance: ""
      memory_capacity: ""

    retriever-usvc:
      replicaCount: [1, 2, 4, 8]
      cores_per_instance: ""
      memory_capacity: ""

    redis-vector-db:
      replicaCount: [1, 1, 1, 1]
      cores_per_instance: ""
      memory_capacity: ""

    chatqna-ui:
      replicaCount: [1, 1, 1, 1]

    nginx:
      replicaCount: [1, 1, 1, 1]

benchmark:
  # http request behavior related fields
  user_queries:              [640]
  concurrency:               [128]
  load_shape_type:           "constant" # "constant" or "poisson"
  poisson_arrival_rate:      1.0  # only used when load_shape_type is "poisson"
  warmup_iterations:         10
  seed:                      1024

  # workload, all of the test cases will run for benchmark
  bench_target: [chatqnafixed, chatqna_qlist_pubmed] # specify the bench_target for benchmark
  dataset: ["/home/sdp/upload_file.txt", "/home/sdp/pubmed_10000.txt"]  # specify the absolute path to the dataset file
  prompt: [10, 1000]  # set the prompt length for the chatqna_qlist_pubmed workload, set to 10 for chatqnafixed workload

  llm:
    # specify the llm output token size
    max_token_size:          [128, 256]
