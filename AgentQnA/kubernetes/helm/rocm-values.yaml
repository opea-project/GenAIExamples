# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Accelerate inferencing in heaviest components to improve performance
# by overriding their subchart values

tgi:
  enabled: false
vllm:
  enabled: true
  accelDevice: "rocm"
  image:
    repository: opea/vllm-rocm
    tag: latest
  env:
    LLM_MODEL_ID: meta-llama/Llama-3.3-70B-Instruct
    HIP_VISIBLE_DEVICES: "0,1"
    TENSOR_PARALLEL_SIZE: "2"
    HF_HUB_DISABLE_PROGRESS_BARS: "1"
    HF_HUB_ENABLE_HF_TRANSFER: "0"
    VLLM_USE_TRITON_FLASH_ATTN: "0"
    VLLM_WORKER_MULTIPROC_METHOD: "spawn"
    PYTORCH_JIT: "0"
    HF_HOME: "/data"
  extraCmd:
    command: [ "python3", "/workspace/api_server.py" ]
  extraCmdArgs: [ "--swap-space", "16",
                  "--disable-log-requests",
                  "--dtype", "float16",
                  "--num-scheduler-steps", "1",
                  "--distributed-executor-backend", "mp" ]
  resources:
    limits:
      amd.com/gpu: "2"
  startupProbe:
    failureThreshold: 180
  securityContext:
    readOnlyRootFilesystem: false
    runAsNonRoot: false
    runAsUser: 0
supervisor:
  llm_endpoint_url: http://{{ .Release.Name }}-vllm
  llm_engine: vllm
  model: "meta-llama/Llama-3.3-70B-Instruct"
ragagent:
  llm_endpoint_url: http://{{ .Release.Name }}-vllm
  llm_engine: vllm
  model: "meta-llama/Llama-3.3-70B-Instruct"
sqlagent:
  llm_endpoint_url: http://{{ .Release.Name }}-vllm
  llm_engine: vllm
  model: "meta-llama/Llama-3.3-70B-Instruct"
