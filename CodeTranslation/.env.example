# Backend API Configuration
BACKEND_PORT=5001

# Inference API Configuration
# INFERENCE_API_ENDPOINT: URL to your inference service (without /v1 suffix)
#   - For GenAI Gateway: https://genai-gateway.example.com
#   - For APISIX Gateway: https://apisix-gateway.example.com/inference
#
# INFERENCE_API_TOKEN: Authentication token/API key for the inference service
#   - For GenAI Gateway: Your GenAI Gateway API key
#   - For APISIX Gateway: Your APISIX authentication token
INFERENCE_API_ENDPOINT=https://your-api-endpoint.com/deployment
INFERENCE_API_TOKEN=your-pre-generated-token-here
INFERENCE_MODEL_NAME=codellama/CodeLlama-34b-Instruct-hf

# LLM Settings
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=4096

# Code Translation Settings
MAX_CODE_LENGTH=10000
MAX_FILE_SIZE=10485760

# CORS Configuration
CORS_ALLOW_ORIGINS=["http://localhost:5173", "http://localhost:3000"]
