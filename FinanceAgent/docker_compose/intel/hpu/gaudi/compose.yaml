# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  worker-finqa-agent:
    image: opea/agent:latest
    container_name: finqa-agent-endpoint
    volumes:
      - ${TOOLSET_PATH}:/home/user/tools/
      - ${PROMPT_PATH}:/home/user/prompts/
    ports:
      - "9095:9095"
    ipc: host
    environment:
      ip_address: ${ip_address}
      strategy: react_llama
      with_memory: false
      recursion_limit: ${recursion_limit_worker}
      llm_engine: vllm
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      llm_endpoint_url: ${LLM_ENDPOINT_URL}
      model: ${LLM_MODEL_ID}
      temperature: ${temperature}
      max_new_tokens: ${max_new_tokens}
      stream: false
      tools: /home/user/tools/finqa_agent_tools.yaml
      custom_prompt: /home/user/prompts/finqa_prompt.py
      require_human_feedback: false
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      port: 9095

  # worker-research-agent:
  #   image: opea/agent:latest
  #   container_name: research-agent-endpoint
  #   volumes:
  #     - ${TOOLSET_PATH}:/home/user/tools/
  #     - ${PROMPT_PATH}:/home/user/prompts/
  #   ports:
  #     - "9096:9096"
  #   ipc: host
  #   environment:
  #     ip_address: ${ip_address}
  #     strategy: react_llama
  #     with_memory: false
  #     recursion_limit: ${recursion_limit_worker}
  #     llm_engine: vllm
  #     HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
  #     llm_endpoint_url: ${LLM_ENDPOINT_URL}
  #     model: ${LLM_MODEL_ID}
  #     temperature: ${temperature}
  #     max_new_tokens: ${max_new_tokens}
  #     stream: false
  #     tools: /home/user/tools/research_agent_tools.yaml
  #     custom_prompt: /home/user/prompts/research_prompt.py
  #     require_human_feedback: false
  #     no_proxy: ${no_proxy}
  #     http_proxy: ${http_proxy}
  #     https_proxy: ${https_proxy}
  #     port: 9096

  supervisor-react-agent:
    image: opea/agent:latest
    container_name: supervisor-agent-endpoint
    depends_on:
      - worker-finqa-agent
      - worker-research-agent
    volumes:
      - ${TOOLSET_PATH}:/home/user/tools/
      - ${PROMPT_PATH}:/home/user/prompts/
    ports:
      - "9090:9090"
    ipc: host
    environment:
      ip_address: ${ip_address}
      strategy: react_llama
      with_memory: true
      recursion_limit: ${recursion_limit_supervisor}
      llm_engine: vllm
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      llm_endpoint_url: ${LLM_ENDPOINT_URL}
      model: ${LLM_MODEL_ID}
      temperature: ${temperature}
      max_new_tokens: ${max_new_tokens}
      stream: true
      tools: /home/user/tools/supervisor_agent_tools.yaml
      custom_prompt: /home/user/prompts/supervisor_prompt.py
      require_human_feedback: false
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      WORKER_FINQA_AGENT_URL: $WORKER_FINQA_AGENT_URL
      WORKER_RESEARCH_AGENT_URL: $WORKER_RESEARCH_AGENT_URL
      port: 9090
