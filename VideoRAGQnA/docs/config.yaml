# Path to all videos
videos: video_ingest/videos/
# Path to video description generated by open-source vision models (ex. video-llama, video-llava, etc.)
description: video_ingest/scene_description/
# Do you want to extract frames of videos (True if not done already, else False)
generate_frames: True
# How do you want to generate feature embeddings?
embeddings:
  type: 'video' # ['video', 'frame']
  path: 'video_ingest/embeddings'
# VL-branch config
vl_branch:
  cfg_path: embedding/video_llama_config/video_llama_eval_only_vl.yaml
  model_type: 'llama_v2'
# Path to store extracted frames
image_output_dir: video_ingest/frames/
# Path to store metadata files
meta_output_dir: video_ingest/frame_metadata/
# Number of frames to extract per second, 
# if 24 fps, and this value is 2, then it will extract 12th and 24th frame
number_of_frames_per_second: 2
# Chunk duration defines the interval of time that each embedding will occur
chunk_duration: 30
# Clip duration defines the length of the interval in which the embeding will occur
clip_duration: 10
# e.g. For every <chunk_duration>, you embed the first <clip_duration>'s frames of that interval

vector_db:
  choice_of_db: 'vdms' #'chroma' # #Supported databases [vdms, chroma]
  host: 0.0.0.0
  port: 55555 #8000 #

adaclip_cfg_path: embedding/adaclip_config/activitynet_filtered.json
adaclip_model_path: #embedding/adaclip_weights/activitynet/2024-04-19_13-57-38/trained_model.pth

# LLM path
model_path: meta-llama/Llama-2-7b-chat-hf
