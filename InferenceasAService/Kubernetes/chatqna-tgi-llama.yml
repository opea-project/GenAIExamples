---
# Source: chatqna/charts/tgi/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: chatqna-tgi-llama-config
  labels:
    helm.sh/chart: tgi-1.0.0
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: chatqna
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
data:
  MODEL_ID: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  PORT: "2080"
  HF_TOKEN: "<your-hf-token>"
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HABANA_LOGS: "/tmp/habana_logs"
  NUMBA_CACHE_DIR: "/tmp"
  HF_HOME: "/tmp/.cache/huggingface"
  MAX_INPUT_LENGTH: "1024"
  MAX_TOTAL_TOKENS: "2048"
---
# Source: chatqna/charts/tgi/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: chatqna-tgi-llama
  labels:
    helm.sh/chart: tgi-1.0.0
    app.kubernetes.io/name: tgillama
    app.kubernetes.io/instance: tgillama
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 2080
      protocol: TCP
      name: tgillama
  selector:
    app.kubernetes.io/name: tgillama
    app.kubernetes.io/instance: tgillama
---
# Source: chatqna/charts/tgi/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: chatqna-tgi-llama
  labels:
    helm.sh/chart: tgi-1.0.0
    app.kubernetes.io/name: tgillama
    app.kubernetes.io/instance: tgillama
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # use explicit replica counts only of HorizontalPodAutoscaler is disabled
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tgillama
      app.kubernetes.io/instance: tgillama
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tgillama
        app.kubernetes.io/instance: tgillama
    spec:
      securityContext:
        {}
      containers:
        - name: tgi
          envFrom:
            - configMapRef:
                name: chatqna-tgi-llama-config
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          image: "ghcr.io/huggingface/tgi-gaudi:2.0.5"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /data
              name: model-volume
            - mountPath: /tmp
              name: tmp
          ports:
            - name: http
              containerPort: 2080
              protocol: TCP
          livenessProbe:
            failureThreshold: 24
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          startupProbe:
            failureThreshold: 120
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          resources:
            limits:
              habana.ai/gaudi: 1
      volumes:
        - name: model-volume
          emptyDir: {}
        - name: tmp
          emptyDir: {}