# ================================================
# PolyLingua Environment Configuration
# ================================================
# Copy this file to .env and update with your values
# Run: cp .env.example .env
# Then edit .env with your actual configuration

# ================================================
# HuggingFace Configuration
# ================================================
# Required: Get your token from https://huggingface.co/settings/tokens
# This is needed to download models from HuggingFace Hub
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# ================================================
# Model Configuration
# ================================================
# LLM model ID from HuggingFace
# Default model supports multilingual translation
LLM_MODEL_ID=swiss-ai/Apertus-8B-Instruct-2509

# Directory to cache downloaded models
# Models can be large (several GB), ensure sufficient disk space
MODEL_CACHE=./data

# ================================================
# Host Configuration
# ================================================
# Your server/machine IP address
# Use 'localhost' for local development
# Use actual IP (e.g., 192.168.1.100) for network access
host_ip=localhost

# ================================================
# Backend Service Configuration
# ================================================
# vLLM (vLLM Inference) endpoint
# This is the LLM inference service endpoint
VLLM_ENDPOINT=http://localhost:8028

# LLM microservice configuration
# Host and port for the LLM microservice
LLM_SERVICE_HOST_IP=localhost
LLM_SERVICE_PORT=9000

# PolyLingua megaservice configuration
# Main translation service host and port
MEGA_SERVICE_HOST_IP=localhost
MEGA_SERVICE_PORT=8888

# Backend service details
BACKEND_SERVICE_NAME=polylingua
BACKEND_SERVICE_IP=localhost
BACKEND_SERVICE_PORT=8888

# ================================================
# Frontend Configuration
# ================================================
# Backend endpoint URL for the frontend
# This is what the UI uses to connect to the backend
BACKEND_SERVICE_ENDPOINT=http://localhost:8888

# Frontend service configuration
# Next.js development server configuration
FRONTEND_SERVICE_IP=localhost
FRONTEND_SERVICE_PORT=5173

# ================================================
# Docker Configuration
# ================================================
# Docker registry for pulling images
# Use 'opea' for official OPEA images
REGISTRY=opea

# Docker image tag
# Use 'latest' for most recent version
TAG=latest

# ================================================
# Nginx Configuration
# ================================================
# Nginx reverse proxy port
# Default HTTP port
NGINX_PORT=80

# ================================================
# Proxy Settings (Optional)
# ================================================
# Configure if behind a corporate proxy
# Leave empty if not using a proxy

# HTTP proxy URL (e.g., http://proxy.company.com:8080)
http_proxy=

# HTTPS proxy URL (e.g., http://proxy.company.com:8080)
https_proxy=

# Comma-separated list of hosts to bypass proxy
no_proxy=localhost,127.0.0.1

# ================================================
# Quick Start Guide
# ================================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Edit .env and set your HF_TOKEN
#
# 3. Update host_ip if deploying to network
#    (use actual IP instead of localhost)
#
# 4. Start services:
#    docker compose up -d
#
# 5. Access UI at:
#    http://localhost:5173 (or http://<host_ip>:5173)
#
# ================================================
